# üîÑ PARTE-04: REDIS CACHE MOBILE-FIRST
*Prompt para IA executar via terminal no servidor*

---

## üéØ **CONTEXTO**
- **Servidor**: 144.202.90.55
- **Objetivo**: Configurar Redis otimizado para sess√µes mobile e cache preditivo
- **Depend√™ncias**: PostgreSQL, MinIO funcionando
- **Login Master**: kryonix / Vitor@123456

---

## üöÄ **EXECUTE ESTES COMANDOS**

```bash
# === CONECTAR NO SERVIDOR ===
ssh root@144.202.90.55

# === VERIFICA√á√ïES INICIAIS ===
echo "üîç Verificando Redis..."
docker ps | grep redis-kryonix
docker exec redis-kryonix redis-cli ping

# === OTIMIZAR CONFIGURA√á√ïES REDIS ===
echo "‚ö° Otimizando Redis para mobile SaaS..."
docker exec redis-kryonix redis-cli << 'EOF'
# Configura√ß√µes otimizadas para mobile
CONFIG SET maxmemory 2gb
CONFIG SET maxmemory-policy allkeys-lru
CONFIG SET timeout 300
CONFIG SET tcp-keepalive 60
CONFIG SET save "900 1 300 10 60 10000"
CONFIG SET appendonly yes
CONFIG SET appendfsync everysec
CONFIG SET auto-aof-rewrite-percentage 100
CONFIG SET auto-aof-rewrite-min-size 64mb

# Otimiza√ß√µes espec√≠ficas mobile
CONFIG SET hash-max-ziplist-entries 512
CONFIG SET hash-max-ziplist-value 64
CONFIG SET list-max-ziplist-size -2
CONFIG SET set-max-intset-entries 512
CONFIG SET zset-max-ziplist-entries 128
CONFIG SET zset-max-ziplist-value 64

# Configura√ß√µes de rede para mobile
CONFIG SET tcp-backlog 511
CONFIG SET client-output-buffer-limit "normal 0 0 0 slave 268435456 67108864 60 pubsub 33554432 8388608 60"

CONFIG REWRITE
EOF

# === ESTRUTURAR DATABASES ESPECIALIZADOS PARA SDK E MULTI-TENANCY ===
echo "üóÇÔ∏è Estruturando 16 databases especializados para SDK unificado e multi-tenancy..."

# Database 0: Sess√µes mobile multi-tenant
docker exec redis-kryonix redis-cli -n 0 << 'EOF'
# Templates para sess√µes mobile isoladas por cliente
HSET mobile:session:template user_id "uuid" device_type "mobile" platform "android|ios" push_token "fcm_token" last_activity "timestamp" geolocation "lat,lng" app_version "1.0.0" language "pt-BR" timezone "America/Sao_Paulo" client_id "tenant_uuid" tenant_namespace "cliente_isolado"

# Sess√µes SDK por cliente isolado
HSET sdk:session:template client_id "tenant_uuid" api_key "sk_cliente_abc123" sdk_version "1.0.0" modules_enabled '["crm","whatsapp","agendamento"]' last_api_call "timestamp" rate_limit_remaining "1000" subscription_status "active"

# Configurar TTL padr√£o para sess√µes (30 minutos)
CONFIG SET tcp-keepalive 1800
EOF

# Database 1: Cache de dados multi-tenant
docker exec redis-kryonix redis-cli -n 1 << 'EOF'
# Templates para cache de dados isolados por cliente
HSET cache:user:template profile_data '{"name":"","email":"","phone":""}' preferences '{"language":"pt-BR","theme":"light","notifications":true}' last_sync "timestamp" cache_version "1.0" client_id "tenant_uuid" tenant_isolation "true"

# Cache de API responses do SDK modular (8 APIs)
HSET cache:api:crm:template endpoint "/api/crm/leads" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "3600" client_id "tenant_uuid"
HSET cache:api:whatsapp:template endpoint "/api/whatsapp/messages" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "1800" client_id "tenant_uuid"
HSET cache:api:agendamento:template endpoint "/api/agendamento/slots" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "900" client_id "tenant_uuid"
HSET cache:api:financeiro:template endpoint "/api/financeiro/cobrancas" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "3600" client_id "tenant_uuid"
HSET cache:api:marketing:template endpoint "/api/marketing/campanhas" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "7200" client_id "tenant_uuid"
HSET cache:api:analytics:template endpoint "/api/analytics/dashboard" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "1800" client_id "tenant_uuid"
HSET cache:api:portal:template endpoint "/api/portal/configuracao" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "3600" client_id "tenant_uuid"
HSET cache:api:whitelabel:template endpoint "/api/whitelabel/branding" response '{"data":[],"meta":{}}' cached_at "timestamp" ttl "86400" client_id "tenant_uuid"
EOF

# Database 2: Filas de trabalho
docker exec redis-kryonix redis-cli -n 2 << 'EOF'
# Fila para WhatsApp
LPUSH queue:whatsapp '{"number":"5517981805327","message":"Teste","priority":"high","scheduled_for":"now"}'

# Fila para notifica√ß√µes push
LPUSH queue:push '{"user_id":"uuid","title":"Notifica√ß√£o","body":"Teste mobile","data":{},"platform":"android"}'

# Fila para processamento de imagens
LPUSH queue:images '{"file_path":"/uploads/image.jpg","operations":["resize","optimize","watermark"],"user_id":"uuid"}'

# Fila para backup autom√°tico
LPUSH queue:backup '{"type":"postgresql","databases":["kryonix_platform"],"scheduled":"02:00","priority":"medium"}'

# Fila para email marketing
LPUSH queue:email '{"campaign_id":"uuid","recipients":[],"template":"welcome","scheduled_for":"timestamp"}'
EOF

# Database 3: M√©tricas em tempo real
docker exec redis-kryonix redis-cli -n 3 << 'EOF'
# Contadores mobile em tempo real
INCR metrics:mobile:active_users
INCR metrics:mobile:api_calls
INCR metrics:mobile:push_sent
INCR metrics:mobile:sessions_created
INCR metrics:mobile:errors_count

# M√©tricas por minuto (ZADD timestamp score)
ZADD metrics:mobile:users_per_minute $(date +%s) "100"
ZADD metrics:mobile:requests_per_minute $(date +%s) "1500"
ZADD metrics:mobile:response_time $(date +%s) "145"

# Hash de estat√≠sticas atuais
HSET metrics:realtime mobile_users "245" api_calls_sec "12" cache_hit_rate "87.5" response_time_ms "145" cpu_usage "23.5" memory_usage "67.2"
EOF

# Database 4: Cache da IA para an√°lise de clientes e cria√ß√£o autom√°tica
docker exec redis-kryonix redis-cli -n 4 << 'EOF'
# Cache de decis√µes IA para multi-tenancy
HSET ai:decisions:template decision_id "uuid" user_id "uuid" client_id "tenant_uuid" context "mobile_login" decision "allow" confidence "0.95" timestamp "timestamp" reasoning "Normal login pattern" tenant_isolation "true"

# Cache de an√°lise de prompts de clientes (FLUXO COMPLETO)
HSET ai:prompt_analysis:template client_request "Preciso de CRM + agenda para minha cl√≠nica" identified_modules '["crm","agendamento","whatsapp","financeiro"]' business_sector "medicina" custom_terminology '{"clientes":"pacientes","vendas":"consultas"}' estimated_price "557" confidence "0.92" analysis_time "2.3s"

# Cache de configura√ß√µes autom√°ticas por setor
HSET ai:sector_config:clinica modules '["crm","agendamento","whatsapp","financeiro"]' terminology '{"leads":"pacientes","sales":"consultas","products":"procedimentos"}' workflows '["agendamento_consulta","lembrete_whatsapp","cobranca_automatica"]' compliance '["lgpd_dados_medicos"]'

HSET ai:sector_config:imobiliaria modules '["crm","whatsapp","agendamento","portal"]' terminology '{"leads":"interessados","products":"imoveis","meetings":"visitas"}' workflows '["qualificacao_leads","agendamento_visitas","follow_up"]' compliance '["lgpd_dados_pessoais"]'

HSET ai:sector_config:salao modules '["agendamento","financeiro","whatsapp","crm"]' terminology '{"leads":"clientes","products":"servicos","staff":"profissionais"}' workflows '["agendamento_servico","lembrete_24h","programa_fidelidade"]' compliance '["lgpd_dados_pessoais"]'

# Cache de cria√ß√£o autom√°tica de plataformas (FLUXO COMPLETO)
HSET ai:platform_creation:template client_id "uuid" business_name "Cl√≠nica Exemplo" sector "medicina" modules_selected '["crm","agendamento","whatsapp"]' subdomain "clinica-exemplo.kryonix.com.br" database_name "kryonix_cliente_clinica_exemplo" creation_time "3min_12s" status "completed" automation_level "100%"

# Padr√µes de comportamento analisados pela IA
HSET ai:patterns:template user_id "uuid" client_id "tenant_uuid" device_type "mobile" usage_pattern "evening_user" frequency "daily" last_analysis "timestamp" business_context "sector_specific"
EOF

# Database 5: Notifica√ß√µes push
docker exec redis-kryonix redis-cli -n 5 << 'EOF'
# Templates para notifica√ß√µes
HSET notification:template:welcome user_id "uuid" title "Bem-vindo ao KRYONIX!" body "Sua plataforma est√° pronta" action_url "https://app.kryonix.com.br" icon_url "/icons/welcome.png"

HSET notification:template:alert user_id "uuid" title "Alerta do Sistema" body "A√ß√£o necess√°ria" priority "high" category "system" sound "default"

# Filas de notifica√ß√µes por plataforma
LPUSH notifications:android '{"user_id":"uuid","message":"Teste Android"}'
LPUSH notifications:ios '{"user_id":"uuid","message":"Teste iOS"}'
LPUSH notifications:web '{"user_id":"uuid","message":"Teste Web"}'
EOF

# Database 6: Cache de API
docker exec redis-kryonix redis-cli -n 6 << 'EOF'
# Cache de responses API mais frequentes
SET api:/users/profile '{"id":"uuid","name":"User","email":"user@example.com"}' EX 3600
SET api:/dashboard/stats '{"users":245,"sessions":156,"revenue":12500}' EX 1800
SET api:/notifications/count '{"unread":5,"total":25}' EX 300

# Cache de consultas de banco pesadas
SET query:user_analytics:uuid '{"total_sessions":150,"avg_duration":325,"last_login":"2025-01-27"}' EX 7200
EOF

# Database 7: Dados tempor√°rios para cria√ß√£o autom√°tica de clientes
docker exec redis-kryonix redis-cli -n 7 << 'EOF'
# C√≥digos de verifica√ß√£o (WhatsApp, SMS, Email) multi-tenant
SET verify:whatsapp:5517981805327 "123456" EX 300
SET verify:email:user@example.com "654321" EX 600

# Tokens tempor√°rios para SDK
SET temp:upload_token:uuid "upload_token_123" EX 1800
SET temp:reset_password:uuid "reset_token_456" EX 3600
SET temp:api_key_generation:uuid "temp_api_key_789" EX 900

# Cache de formul√°rios de cria√ß√£o de clientes (FLUXO COMPLETO)
SET form:client_creation:uuid '{"business_name":"Cl√≠nica Exemplo","owner_name":"Dr. Jo√£o","phone":"+5517981805327","email":"dr.joao@clinica.com","sector":"medicina","prompt":"Preciso de CRM + agenda para minha cl√≠nica"}' EX 1800

# Cache de configura√ß√µes durante cria√ß√£o autom√°tica
SET config:auto_creation:uuid '{"step":"analyzing_prompt","progress":"25%","estimated_time":"2min","modules_identified":["crm","agendamento","whatsapp"],"next_action":"create_database"}' EX 3600

# Cache de credentials durante entrega (FLUXO COMPLETO)
SET credentials:delivery:uuid '{"subdomain":"clinica-exemplo.kryonix.com.br","api_key":"sk_clinica_exemplo_abc123","admin_user":"admin@clinica-exemplo.com","temp_password":"TempPass123!","qr_whatsapp":"https://api.kryonix.com.br/clinica-exemplo/whatsapp/qr"}' EX 7200
EOF

# Database 8: Geolocaliza√ß√£o multi-tenant
docker exec redis-kryonix redis-cli -n 8 << 'EOF'
# Cache de localiza√ß√µes de usu√°rios isoladas por cliente
GEOADD locations:users:cliente1 -46.6333 -23.5505 user1 -47.0608 -22.9068 user2
GEOADD locations:users:cliente2 -46.6333 -23.5505 user3 -47.0608 -22.9068 user4

# Localiza√ß√µes de estabelecimentos por cliente
GEOADD locations:stores:cliente1 -46.6333 -23.5505 store1 -47.0608 -22.9068 store2
GEOADD locations:stores:cliente2 -46.6333 -23.5505 store3 -47.0608 -22.9068 store4

# Cache de endere√ßos
SET address:cep:17500000 '{"street":"Rua Example","city":"Mar√≠lia","state":"SP"}' EX 86400
EOF

# Database 9: SDK e API Keys por cliente
docker exec redis-kryonix redis-cli -n 9 << 'EOF'
# SDK configurations por cliente (ARQUITETURA SDK)
HSET sdk:config:cliente1 api_key "sk_cliente1_abc123" modules_enabled '["crm","whatsapp","agendamento"]' rate_limit "1000/min" subscription_status "active" client_name "Cl√≠nica Exemplo" subdomain "clinica-exemplo.kryonix.com.br"

HSET sdk:config:cliente2 api_key "sk_cliente2_def456" modules_enabled '["crm","whatsapp","agendamento","financeiro"]' rate_limit "2000/min" subscription_status "active" client_name "Siqueira Campos Im√≥veis" subdomain "siqueiracampos.kryonix.com.br"

# SDK usage metrics por cliente
HSET sdk:usage:cliente1 api_calls_today "1250" api_calls_month "45000" last_call "timestamp" most_used_module "whatsapp" active_sessions "25"

# SDK versions por cliente
HSET sdk:versions:cliente1 current_version "1.2.5" last_update "2025-01-27" auto_update "true" npm_package "@kryonix/sdk@1.2.5"
EOF

# Database 10: Multi-tenancy management
docker exec redis-kryonix redis-cli -n 10 << 'EOF'
# Tenant configurations (MULTI-TENANCY)
HSET tenant:cliente1 tenant_id "uuid_cliente1" business_name "Cl√≠nica Exemplo" database_name "kryonix_cliente_clinica_exemplo" subdomain "clinica-exemplo.kryonix.com.br" status "active" created_at "2025-01-27" sector "medicina" modules '["crm","agendamento","whatsapp","financeiro"]'

HSET tenant:cliente2 tenant_id "uuid_cliente2" business_name "Siqueira Campos Im√≥veis" database_name "kryonix_cliente_siqueira_campos" subdomain "siqueiracampos.kryonix.com.br" status "active" created_at "2025-01-26" sector "imobiliario" modules '["crm","whatsapp","agendamento","portal"]'

# Payment status por tenant
HSET payment:cliente1 status "paid" due_date "2025-02-27" amount "557.00" method "pix" auto_renewal "true"
HSET payment:cliente2 status "paid" due_date "2025-02-26" amount "897.00" method "credit_card" auto_renewal "true"

# Tenant isolation validation
HSET isolation:validation tenant_access_control "strict" cross_tenant_access "forbidden" data_encryption "enabled" backup_isolation "enabled"
EOF

# Database 11: Apps Mobile por cliente (APPS MOBILE)
docker exec redis-kryonix redis-cli -n 11 << 'EOF'
# Mobile apps configurations por cliente
HSET mobile_apps:cliente1 android_package "com.kryonix.clinica_exemplo" ios_bundle "com.kryonix.clinicaexemplo" app_name "Cl√≠nica Exemplo" logo_url "https://s3.kryonix.com.br/cliente1/logo.png" primary_color "#2E7D32" secondary_color "#81C784" download_page "https://downloads.kryonix.com.br/cliente1" last_build "2025-01-27" auto_build "true"

# App distribution status
HSET app_distribution:cliente1 apk_ready "true" apk_url "https://downloads.kryonix.com.br/cliente1/android.apk" ipa_ready "true" ipa_url "https://downloads.kryonix.com.br/cliente1/ios.ipa" pwa_url "https://clinica-exemplo.kryonix.com.br" play_store_status "not_published" app_store_status "not_published"

# App usage metrics
HSET app_usage:cliente1 android_downloads "156" ios_downloads "89" pwa_installs "234" daily_active_users "67" session_duration "8min" retention_rate "78%"
EOF

# === IMPLEMENTAR CACHE PREDITIVO COM IA ===
echo "ü§ñ Implementando cache preditivo com IA..."
cat > /opt/kryonix/scripts/redis-ai-predictive.py << 'EOF'
#!/usr/bin/env python3
import redis
import json
import numpy as np
from datetime import datetime, timedelta
import pickle
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RedisMultiTenantCachePredictive:
    def __init__(self):
        self.r = redis.Redis(host='redis-kryonix', port=6379, decode_responses=True)
        self.patterns_db = 4  # Database para padr√µes IA
        self.sdk_db = 9      # Database para SDK
        self.tenant_db = 10  # Database para multi-tenancy
        self.apps_db = 11    # Database para apps mobile
        
    def analyze_tenant_patterns(self):
        """IA analisa padr√µes de uso por tenant e SDK"""
        try:
            patterns = {}
            
            # Analisar sess√µes mobile (Database 0)
            self.r.select(0)
            mobile_keys = self.r.keys('mobile:session:*')
            
            for key in mobile_keys:
                session_data = self.r.hgetall(key)
                if session_data:
                    user_id = session_data.get('user_id')
                    device_type = session_data.get('device_type')
                    platform = session_data.get('platform')
                    last_activity = session_data.get('last_activity')
                    
                    # IA identifica padr√µes por plataforma
                    pattern_key = f"pattern:{platform}:{device_type}"
                    if pattern_key not in patterns:
                        patterns[pattern_key] = {
                            'users': [],
                            'peak_hours': [],
                            'avg_session_duration': 0,
                            'common_actions': []
                        }
                    
                    patterns[pattern_key]['users'].append(user_id)
                    
                    # Analisar hor√°rio de pico
                    if last_activity:
                        try:
                            hour = datetime.fromisoformat(last_activity).hour
                            patterns[pattern_key]['peak_hours'].append(hour)
                        except:
                            pass
            
            # Salvar padr√µes para IA (Database 4)
            self.r.select(4)
            for pattern_key, data in patterns.items():
                # Calcular estat√≠sticas
                if data['peak_hours']:
                    peak_hour = max(set(data['peak_hours']), key=data['peak_hours'].count)
                    data['most_active_hour'] = peak_hour
                
                data['total_users'] = len(set(data['users']))
                data['analyzed_at'] = datetime.now().isoformat()
                
                self.r.setex(pattern_key, 86400, json.dumps(data))
            
            logger.info(f"Padr√µes analisados: {len(patterns)}")
            return patterns
            
        except Exception as e:
            logger.error(f"Erro na an√°lise de padr√µes: {e}")
            return {}
    
    def predict_cache_needs(self):
        """IA prediz necessidades de cache"""
        try:
            self.r.select(4)
            pattern_keys = self.r.keys('pattern:*')
            
            predictions = []
            
            for key in pattern_keys:
                try:
                    pattern_data = json.loads(self.r.get(key) or '{}')
                    
                    if pattern_data.get('total_users', 0) > 5:
                        # IA prediz pico de uso
                        peak_hour = pattern_data.get('most_active_hour', 12)
                        current_hour = datetime.now().hour
                        
                        # Se estivermos pr√≥ximos do pico, pre-cache dados
                        hours_to_peak = (peak_hour - current_hour) % 24
                        
                        if hours_to_peak <= 1:  # 1 hora antes do pico
                            prediction = {
                                'pattern': key,
                                'action': 'pre_cache_user_data',
                                'priority': 'high',
                                'estimated_users': pattern_data['total_users'],
                                'peak_hour': peak_hour,
                                'confidence': 0.85
                            }
                            predictions.append(prediction)
                
                except Exception as e:
                    logger.warning(f"Erro ao processar padr√£o {key}: {e}")
            
            logger.info(f"Predi√ß√µes geradas: {len(predictions)}")
            return predictions
            
        except Exception as e:
            logger.error(f"Erro nas predi√ß√µes: {e}")
            return []
    
    def auto_cache_optimization(self):
        """IA otimiza cache automaticamente"""
        try:
            # Analisar hit rate geral
            info = self.r.info()
            keyspace_hits = info.get('keyspace_hits', 0)
            keyspace_misses = info.get('keyspace_misses', 0)
            
            if keyspace_hits + keyspace_misses > 0:
                hit_rate = keyspace_hits / (keyspace_hits + keyspace_misses)
            else:
                hit_rate = 0
            
            logger.info(f"Cache Hit Rate atual: {hit_rate:.2%}")
            
            optimizations_made = []
            
            # Se hit rate baixo, IA otimiza
            if hit_rate < 0.8:
                logger.info("Hit rate baixo, iniciando otimiza√ß√µes...")
                
                # Reorganizar cache por prioridade
                optimizations_made.extend(self.reorganize_cache_by_priority())
                
                # Pre-cache dados frequentes
                optimizations_made.extend(self.pre_cache_frequent_data())
            
            # Limpar dados expirados
            optimizations_made.extend(self.cleanup_expired_data())
            
            # Salvar m√©tricas de otimiza√ß√£o
            self.r.select(3)
            self.r.hset('metrics:cache_optimization', 
                       'last_run', datetime.now().isoformat(),
                       'hit_rate', f"{hit_rate:.3f}",
                       'optimizations_count', len(optimizations_made))
            
            logger.info(f"Otimiza√ß√µes executadas: {len(optimizations_made)}")
            return optimizations_made
            
        except Exception as e:
            logger.error(f"Erro na otimiza√ß√£o: {e}")
            return []
    
    def reorganize_cache_by_priority(self):
        """IA reorganiza cache por prioridade de uso"""
        actions = []
        
        try:
            # Database 0: Sess√µes mobile - prioridade alta
            self.r.select(0)
            session_keys = self.r.keys('mobile:session:*')
            
            for key in session_keys:
                ttl = self.r.ttl(key)
                if ttl < 300:  # Menos de 5 minutos
                    # Estender sess√µes ativas
                    self.r.expire(key, 1800)  # 30 minutos
                    actions.append(f"Extended session TTL: {key}")
            
            # Database 1: Cache de dados - otimizar TTL
            self.r.select(1)
            cache_keys = self.r.keys('cache:*')
            
            for key in cache_keys:
                ttl = self.r.ttl(key)
                if ttl < 60:  # Menos de 1 minuto
                    # Re-cache dados importantes
                    self.r.expire(key, 3600)  # 1 hora
                    actions.append(f"Re-cached important data: {key}")
            
        except Exception as e:
            logger.error(f"Erro na reorganiza√ß√£o: {e}")
        
        return actions
    
    def pre_cache_frequent_data(self):
        """IA pre-carrega dados frequentemente acessados"""
        actions = []
        
        try:
            # Pre-cache dados de usu√°rios ativos
            self.r.select(1)
            
            # Simular pre-cache de dados do dashboard
            dashboard_data = {
                'total_users': 245,
                'active_sessions': 156,
                'revenue_today': 12500,
                'notifications_pending': 23
            }
            
            self.r.setex('cache:dashboard:main', 1800, json.dumps(dashboard_data))
            actions.append("Pre-cached dashboard data")
            
            # Pre-cache configura√ß√µes do sistema
            system_config = {
                'maintenance_mode': False,
                'api_version': '1.0',
                'features_enabled': ['push_notifications', 'geolocation', 'ai_recommendations']
            }
            
            self.r.setex('cache:system:config', 3600, json.dumps(system_config))
            actions.append("Pre-cached system config")
            
        except Exception as e:
            logger.error(f"Erro no pre-cache: {e}")
        
        return actions
    
    def cleanup_expired_data(self):
        """IA limpa dados expirados automaticamente"""
        actions = []
        
        try:
            # Database 7: Dados tempor√°rios - limpar expirados
            self.r.select(7)
            
            # Limpar c√≥digos de verifica√ß√£o expirados
            verify_keys = self.r.keys('verify:*')
            expired_count = 0
            
            for key in verify_keys:
                ttl = self.r.ttl(key)
                if ttl <= 0:  # Expirado
                    self.r.delete(key)
                    expired_count += 1
            
            if expired_count > 0:
                actions.append(f"Cleaned {expired_count} expired verification codes")
            
            # Limpar tokens tempor√°rios expirados
            temp_keys = self.r.keys('temp:*')
            temp_expired = 0
            
            for key in temp_keys:
                ttl = self.r.ttl(key)
                if ttl <= 0:
                    self.r.delete(key)
                    temp_expired += 1
            
            if temp_expired > 0:
                actions.append(f"Cleaned {temp_expired} expired temporary tokens")
                
        except Exception as e:
            logger.error(f"Erro na limpeza: {e}")
        
        return actions
    
    def monitor_performance(self):
        """IA monitora performance do Redis"""
        try:
            info = self.r.info()
            
            performance_metrics = {
                'used_memory_mb': round(info.get('used_memory', 0) / 1024 / 1024, 2),
                'connected_clients': info.get('connected_clients', 0),
                'total_commands_processed': info.get('total_commands_processed', 0),
                'keyspace_hits': info.get('keyspace_hits', 0),
                'keyspace_misses': info.get('keyspace_misses', 0),
                'expired_keys': info.get('expired_keys', 0),
                'evicted_keys': info.get('evicted_keys', 0)
            }
            
            # Calcular hit rate
            hits = performance_metrics['keyspace_hits']
            misses = performance_metrics['keyspace_misses']
            
            if hits + misses > 0:
                performance_metrics['hit_rate'] = round(hits / (hits + misses), 3)
            else:
                performance_metrics['hit_rate'] = 0
            
            # Salvar m√©tricas
            self.r.select(3)
            self.r.hset('metrics:redis_performance', 
                       mapping={k: str(v) for k, v in performance_metrics.items()})
            self.r.hset('metrics:redis_performance', 'timestamp', datetime.now().isoformat())
            
            logger.info(f"M√©tricas de performance: {performance_metrics}")
            
            # Alertas autom√°ticos
            alerts = []
            
            if performance_metrics['used_memory_mb'] > 1500:  # > 1.5GB
                alerts.append(f"Alto uso de mem√≥ria: {performance_metrics['used_memory_mb']}MB")
            
            if performance_metrics['connected_clients'] > 180:
                alerts.append(f"Muitas conex√µes: {performance_metrics['connected_clients']}")
            
            if performance_metrics['hit_rate'] < 0.7:
                alerts.append(f"Hit rate baixo: {performance_metrics['hit_rate']:.1%}")
            
            return performance_metrics, alerts
            
        except Exception as e:
            logger.error(f"Erro no monitoramento: {e}")
            return {}, []

    def analyze_sdk_usage_patterns(self):
        """IA analisa padr√µes de uso do SDK por cliente"""
        try:
            self.r.select(self.sdk_db)
            sdk_configs = self.r.keys('sdk:config:*')

            patterns = {}

            for config_key in sdk_configs:
                client_id = config_key.split(':')[2]
                sdk_data = self.r.hgetall(config_key)

                if sdk_data:
                    # Analisar uso do SDK por cliente
                    usage_key = f'sdk:usage:{client_id}'
                    usage_data = self.r.hgetall(usage_key)

                    patterns[client_id] = {
                        'modules_enabled': sdk_data.get('modules_enabled', '[]'),
                        'api_calls_today': int(usage_data.get('api_calls_today', 0)),
                        'most_used_module': usage_data.get('most_used_module', 'unknown'),
                        'rate_limit': sdk_data.get('rate_limit', '1000/min'),
                        'subscription_status': sdk_data.get('subscription_status', 'unknown'),
                        'active_sessions': int(usage_data.get('active_sessions', 0))
                    }

            # Salvar an√°lise para IA
            self.r.select(self.patterns_db)
            self.r.setex('sdk:usage_analysis', 86400, json.dumps({
                'timestamp': datetime.now().isoformat(),
                'total_clients': len(patterns),
                'patterns': patterns,
                'recommendations': self.generate_sdk_recommendations(patterns)
            }))

            logger.info(f"Padr√µes SDK analisados para {len(patterns)} clientes")
            return patterns

        except Exception as e:
            logger.error(f"Erro na an√°lise SDK: {e}")
            return {}

    def generate_sdk_recommendations(self, patterns):
        """IA gera recomenda√ß√µes baseadas no uso do SDK"""
        recommendations = []

        for client_id, data in patterns.items():
            # Cliente com muitas chamadas API
            if data['api_calls_today'] > 5000:
                recommendations.append(f"Cliente {client_id}: Considerar upgrade de plano")

            # Cliente com poucas sess√µes ativas
            if data['active_sessions'] < 5:
                recommendations.append(f"Cliente {client_id}: Baixo engajamento - revisar onboarding")

            # M√≥dulo mais usado
            if data['most_used_module'] == 'whatsapp':
                recommendations.append(f"Cliente {client_id}: Focar otimiza√ß√µes WhatsApp")

        return recommendations

    def analyze_automatic_client_creation(self):
        """IA analisa cria√ß√µes autom√°ticas de clientes (FLUXO COMPLETO)"""
        try:
            self.r.select(self.patterns_db)
            creation_keys = self.r.keys('ai:platform_creation:*')

            creation_analytics = {
                'total_created_today': 0,
                'average_creation_time': 0,
                'success_rate': 0,
                'popular_sectors': {},
                'popular_modules': {}
            }

            today = datetime.now().date()
            successful_creations = []

            for key in creation_keys:
                creation_data = self.r.hgetall(key)
                if creation_data:
                    # Verificar se foi criado hoje
                    if creation_data.get('status') == 'completed':
                        creation_time = creation_data.get('creation_time', '0min')
                        # Extrair tempo em minutos
                        minutes = float(creation_time.replace('min', '').replace('s', '').split('_')[0])
                        successful_creations.append(minutes)

                        # Analisar setor
                        sector = creation_data.get('sector', 'unknown')
                        creation_analytics['popular_sectors'][sector] = creation_analytics['popular_sectors'].get(sector, 0) + 1

                        # Analisar m√≥dulos
                        modules = json.loads(creation_data.get('modules_selected', '[]'))
                        for module in modules:
                            creation_analytics['popular_modules'][module] = creation_analytics['popular_modules'].get(module, 0) + 1

            if successful_creations:
                creation_analytics['total_created_today'] = len(successful_creations)
                creation_analytics['average_creation_time'] = sum(successful_creations) / len(successful_creations)
                creation_analytics['success_rate'] = len(successful_creations) / len(creation_keys) * 100

            # Salvar an√°lise
            self.r.setex('ai:creation_analytics', 86400, json.dumps({
                'timestamp': datetime.now().isoformat(),
                'analytics': creation_analytics,
                'insights': [
                    f"Tempo m√©dio de cria√ß√£o: {creation_analytics['average_creation_time']:.1f} minutos",
                    f"Taxa de sucesso: {creation_analytics['success_rate']:.1f}%",
                    f"Setor mais popular: {max(creation_analytics['popular_sectors'], key=creation_analytics['popular_sectors'].get) if creation_analytics['popular_sectors'] else 'N/A'}",
                    f"M√≥dulo mais solicitado: {max(creation_analytics['popular_modules'], key=creation_analytics['popular_modules'].get) if creation_analytics['popular_modules'] else 'N/A'}"
                ]
            }))

            logger.info(f"An√°lise de cria√ß√£o autom√°tica: {creation_analytics['total_created_today']} clientes criados")
            return creation_analytics

        except Exception as e:
            logger.error(f"Erro na an√°lise de cria√ß√£o autom√°tica: {e}")
            return {}

    def monitor_multi_tenant_health(self):
        """IA monitora sa√∫de do sistema multi-tenant"""
        try:
            self.r.select(self.tenant_db)
            tenant_keys = self.r.keys('tenant:*')

            health_report = {
                'total_tenants': len(tenant_keys),
                'active_tenants': 0,
                'payment_issues': 0,
                'resource_usage': {},
                'isolation_status': 'healthy'
            }

            for tenant_key in tenant_keys:
                tenant_data = self.r.hgetall(tenant_key)
                client_id = tenant_key.split(':')[1]

                if tenant_data.get('status') == 'active':
                    health_report['active_tenants'] += 1

                # Verificar status de pagamento
                payment_key = f'payment:{client_id}'
                payment_data = self.r.hgetall(payment_key)

                if payment_data.get('status') != 'paid':
                    health_report['payment_issues'] += 1

                # Verificar uso de recursos
                self.r.select(self.sdk_db)
                usage_key = f'sdk:usage:{client_id}'
                usage_data = self.r.hgetall(usage_key)

                if usage_data:
                    api_calls = int(usage_data.get('api_calls_today', 0))
                    health_report['resource_usage'][client_id] = api_calls

            # Calcular scores
            if health_report['total_tenants'] > 0:
                health_report['health_score'] = (
                    (health_report['active_tenants'] / health_report['total_tenants']) * 50 +
                    ((health_report['total_tenants'] - health_report['payment_issues']) / health_report['total_tenants']) * 50
                )
            else:
                health_report['health_score'] = 100

            # Salvar relat√≥rio
            self.r.select(self.patterns_db)
            self.r.setex('multi_tenant:health', 3600, json.dumps({
                'timestamp': datetime.now().isoformat(),
                'health_report': health_report,
                'alerts': [
                    f"Problemas de pagamento: {health_report['payment_issues']} clientes" if health_report['payment_issues'] > 0 else None,
                    f"Score de sa√∫de: {health_report['health_score']:.1f}%"
                ]
            }))

            logger.info(f"Health check multi-tenant: {health_report['active_tenants']}/{health_report['total_tenants']} ativos")
            return health_report

        except Exception as e:
            logger.error(f"Erro no monitoramento multi-tenant: {e}")
            return {}

def main():
    cache_ai = RedisMultiTenantCachePredictive()
    
    try:
        logger.info("ü§ñ IA Redis Multi-Tenant iniciando...")

        # Analisar padr√µes mobile
        patterns = cache_ai.analyze_tenant_patterns()

        # Analisar uso do SDK
        sdk_patterns = cache_ai.analyze_sdk_usage_patterns()

        # Analisar cria√ß√µes autom√°ticas
        creation_analytics = cache_ai.analyze_automatic_client_creation()

        # Monitorar sa√∫de multi-tenant
        tenant_health = cache_ai.monitor_multi_tenant_health()
        
        # Gerar predi√ß√µes
        predictions = cache_ai.predict_cache_needs()
        
        # Otimizar cache
        optimizations = cache_ai.auto_cache_optimization()
        
        # Monitorar performance
        metrics, alerts = cache_ai.monitor_performance()
        
        # Relat√≥rio final completo
        report = {
            'timestamp': datetime.now().isoformat(),
            'mobile_patterns_analyzed': len(patterns),
            'sdk_clients_analyzed': len(sdk_patterns),
            'predictions_generated': len(predictions),
            'optimizations_made': len(optimizations),
            'performance_metrics': metrics,
            'creation_analytics': creation_analytics,
            'tenant_health': tenant_health,
            'alerts': alerts,
            'multi_tenant_insights': {
                'total_tenants': tenant_health.get('total_tenants', 0),
                'active_tenants': tenant_health.get('active_tenants', 0),
                'health_score': tenant_health.get('health_score', 0),
                'avg_creation_time': f"{creation_analytics.get('average_creation_time', 0):.1f} min"
            },
            'sdk_insights': {
                'total_api_calls': sum(data.get('api_calls_today', 0) for data in sdk_patterns.values()),
                'active_sdk_clients': len([c for c in sdk_patterns.values() if c.get('subscription_status') == 'active']),
                'most_popular_module': max([data.get('most_used_module', 'unknown') for data in sdk_patterns.values()], key=lambda x: list(sdk_patterns.values()).count(x)) if sdk_patterns else 'N/A'
            }
        }
        
        logger.info(f"Relat√≥rio IA Multi-Tenant: {json.dumps(report, indent=2)}")
        
        # Salvar relat√≥rio
        with open('/opt/kryonix/logs/redis-multitenant-ai-report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # Enviar alertas via WhatsApp se necess√°rio
        if alerts:
            alert_message = "‚ö†Ô∏è ALERTAS REDIS:\\n" + "\\n".join(alerts)
            # Aqui seria implementado envio WhatsApp
            logger.warning(f"Alertas gerados: {alerts}")
        
        logger.info("‚úÖ IA Redis Multi-Tenant executada com sucesso")
        
    except Exception as e:
        logger.error(f"‚ùå Erro na execu√ß√£o da IA Multi-Tenant: {e}")

if __name__ == "__main__":
    main()
EOF

chmod +x /opt/kryonix/scripts/redis-ai-predictive.py

# Instalar depend√™ncias Python
pip3 install redis numpy

# === CONFIGURAR BACKUP AUTOM√ÅTICO ===
echo "üíæ Configurando backup autom√°tico Redis..."
cat > /opt/kryonix/scripts/backup-redis.sh << 'EOF'
#!/bin/bash
BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/opt/kryonix/backups/redis/$BACKUP_DATE"
mkdir -p "$BACKUP_DIR"

echo "üíæ Iniciando backup Redis..."

# Backup RDB (snapshot)
echo "üì∏ Criando snapshot RDB..."
docker exec redis-kryonix redis-cli BGSAVE
sleep 10  # Aguardar backup concluir

# Copiar arquivo RDB
docker exec redis-kryonix cp /data/dump.rdb /tmp/
docker cp redis-kryonix:/tmp/dump.rdb "$BACKUP_DIR/redis_dump.rdb"

# Backup AOF (append only file)
echo "üìù Backup AOF..."
if docker exec redis-kryonix ls /data/appendonly.aof >/dev/null 2>&1; then
    docker exec redis-kryonix cp /data/appendonly.aof /tmp/
    docker cp redis-kryonix:/tmp/appendonly.aof "$BACKUP_DIR/redis_appendonly.aof"
fi

# Backup configura√ß√£o
echo "‚öôÔ∏è Backup configura√ß√£o..."
docker exec redis-kryonix redis-cli CONFIG GET "*" > "$BACKUP_DIR/redis_config.txt"

# Backup de cada database
echo "üóÇÔ∏è Backup de databases..."
for db in {0..15}; do
    echo "Database $db:"
    docker exec redis-kryonix redis-cli -n $db KEYS "*" > "$BACKUP_DIR/keys_db${db}.txt"
    docker exec redis-kryonix redis-cli -n $db DBSIZE >> "$BACKUP_DIR/dbsize.txt"
done

# Backup de m√©tricas IA
echo "ü§ñ Backup m√©tricas IA..."
docker exec redis-kryonix redis-cli -n 3 HGETALL metrics:redis_performance > "$BACKUP_DIR/ai_metrics.txt"
docker exec redis-kryonix redis-cli -n 4 KEYS "pattern:*" > "$BACKUP_DIR/ai_patterns.txt"

# Comprimir backup
cd /opt/kryonix/backups/redis
tar -czf "redis_backup_$BACKUP_DATE.tar.gz" "$BACKUP_DATE"
rm -rf "$BACKUP_DATE"

# Limpar backups antigos (manter 7 dias)
find /opt/kryonix/backups/redis -name "redis_backup_*.tar.gz" -mtime +7 -delete

BACKUP_SIZE=$(du -sh "redis_backup_$BACKUP_DATE.tar.gz" | cut -f1)
echo "‚úÖ Backup Redis conclu√≠do: $BACKUP_SIZE"

# Notificar WhatsApp
curl -X POST "https://evolution.kryonix.com.br/message/sendText" \
  -H "apikey: sua_chave_evolution_api_aqui" \
  -H "Content-Type: application/json" \
  -d "{\"number\": \"5517981805327\", \"text\": \"üîÑ Backup Redis conclu√≠do!\\nData: $BACKUP_DATE\\nTamanho: $BACKUP_SIZE\\nDatabases: 16\"}"
EOF

chmod +x /opt/kryonix/scripts/backup-redis.sh

# === CONFIGURAR MONITORAMENTO ===
echo "üìä Configurando monitoramento Redis..."
cat > /opt/kryonix/scripts/monitor-redis.sh << 'EOF'
#!/bin/bash
# Monitoramento cont√≠nuo Redis

while true; do
  # Health check b√°sico
  if ! docker exec redis-kryonix redis-cli ping > /dev/null 2>&1; then
    echo "üö® $(date): Redis n√£o est√° respondendo!"
    
    # Tentar restart
    docker service update --force kryonix-redis_redis
    
    # Notificar WhatsApp
    curl -X POST "https://evolution.kryonix.com.br/message/sendText" \
      -H "apikey: sua_chave_evolution_api_aqui" \
      -H "Content-Type: application/json" \
      -d "{\"number\": \"5517981805327\", \"text\": \"üö® ALERTA: Redis fora do ar!\\nTentando restart autom√°tico...\"}"
  fi
  
  # Verificar uso de mem√≥ria
  MEMORY_USED=$(docker exec redis-kryonix redis-cli info memory | grep used_memory_human | cut -d: -f2 | tr -d '\r')
  MEMORY_PEAK=$(docker exec redis-kryonix redis-cli info memory | grep used_memory_peak_human | cut -d: -f2 | tr -d '\r')
  
  # Verificar conex√µes
  CONNECTED_CLIENTS=$(docker exec redis-kryonix redis-cli info clients | grep connected_clients | cut -d: -f2 | tr -d '\r')
  
  # Verificar hit rate
  KEYSPACE_HITS=$(docker exec redis-kryonix redis-cli info stats | grep keyspace_hits | cut -d: -f2 | tr -d '\r')
  KEYSPACE_MISSES=$(docker exec redis-kryonix redis-cli info stats | grep keyspace_misses | cut -d: -f2 | tr -d '\r')
  
  if [ "$KEYSPACE_HITS" -gt 0 ] && [ "$KEYSPACE_MISSES" -gt 0 ]; then
    HIT_RATE=$(echo "scale=2; $KEYSPACE_HITS / ($KEYSPACE_HITS + $KEYSPACE_MISSES) * 100" | bc)
  else
    HIT_RATE=0
  fi
  
  # Salvar m√©tricas no Redis
  docker exec redis-kryonix redis-cli -n 3 << EOF2
HSET metrics:redis:monitoring memory_used "$MEMORY_USED" memory_peak "$MEMORY_PEAK" connected_clients "$CONNECTED_CLIENTS" hit_rate_percent "$HIT_RATE" timestamp "$(date +%s)"
EOF2
  
  # Alertas
  if [ "$CONNECTED_CLIENTS" -gt 180 ]; then
    curl -X POST "https://evolution.kryonix.com.br/message/sendText" \
      -H "apikey: sua_chave_evolution_api_aqui" \
      -H "Content-Type: application/json" \
      -d "{\"number\": \"5517981805327\", \"text\": \"‚ö†Ô∏è Redis: Muitas conex√µes ($CONNECTED_CLIENTS/200)\"}"
  fi
  
  # Verificar se hit rate est√° muito baixo
  if (( $(echo "$HIT_RATE < 70" | bc -l) )); then
    curl -X POST "https://evolution.kryonix.com.br/message/sendText" \
      -H "apikey: sua_chave_evolution_api_aqui" \
      -H "Content-Type: application/json" \
      -d "{\"number\": \"5517981805327\", \"text\": \"‚ö†Ô∏è Redis: Hit rate baixo (${HIT_RATE}%)\"}"
  fi
  
  echo "‚úÖ $(date): Redis funcionando - Mem√≥ria: $MEMORY_USED, Clientes: $CONNECTED_CLIENTS, Hit Rate: ${HIT_RATE}%"
  
  sleep 300  # 5 minutos
done
EOF

chmod +x /opt/kryonix/scripts/monitor-redis.sh

# Executar monitoramento em background
nohup /opt/kryonix/scripts/monitor-redis.sh > /var/log/redis-monitor.log 2>&1 &

# === CONFIGURAR SCRIPTS DE LIMPEZA ===
echo "üßπ Configurando limpeza autom√°tica..."
cat > /opt/kryonix/scripts/redis-cleanup.sh << 'EOF'
#!/bin/bash
# Limpeza autom√°tica de dados expirados

echo "üßπ Iniciando limpeza autom√°tica Redis..."

# Executar script IA para limpeza inteligente
python3 /opt/kryonix/scripts/redis-ai-predictive.py

# Limpeza manual de dados muito antigos
echo "üóëÔ∏è Limpeza manual de dados antigos..."

# Database 7: Dados tempor√°rios
docker exec redis-kryonix redis-cli -n 7 << 'EOF2'
# Deletar c√≥digos de verifica√ß√£o expirados manualmente
EVAL "
local keys = redis.call('KEYS', 'verify:*')
local deleted = 0
for i=1,#keys do
    local ttl = redis.call('TTL', keys[i])
    if ttl <= 0 then
        redis.call('DEL', keys[i])
        deleted = deleted + 1
    end
end
return deleted
" 0

# Deletar tokens tempor√°rios expirados
EVAL "
local keys = redis.call('KEYS', 'temp:*')
local deleted = 0
for i=1,#keys do
    local ttl = redis.call('TTL', keys[i])
    if ttl <= 0 then
        redis.call('DEL', keys[i])
        deleted = deleted + 1
    end
end
return deleted
" 0
EOF2

echo "‚úÖ Limpeza autom√°tica conclu√≠da"
EOF

chmod +x /opt/kryonix/scripts/redis-cleanup.sh

# === AGENDAR TAREFAS AUTOM√ÅTICAS ===
echo "üìÖ Agendando tarefas autom√°ticas..."
(crontab -l 2>/dev/null; echo "0 3 * * * /opt/kryonix/scripts/backup-redis.sh") | crontab -
(crontab -l 2>/dev/null; echo "*/5 * * * * /usr/bin/python3 /opt/kryonix/scripts/redis-ai-predictive.py >> /var/log/redis-ai.log 2>&1") | crontab -
(crontab -l 2>/dev/null; echo "0 */6 * * * /opt/kryonix/scripts/redis-cleanup.sh >> /var/log/redis-cleanup.log 2>&1") | crontab -

# === CONFIGURAR EXEMPLO DE INTEGRA√á√ÉO MOBILE ===
echo "üì± Criando exemplo de integra√ß√£o mobile..."
cat > /opt/kryonix/scripts/mobile-redis-example.js << 'EOF'
// Exemplo de integra√ß√£o Redis para apps mobile
const redis = require('redis');

class KryonixMobileRedis {
    constructor() {
        this.client = redis.createClient({
            host: 'redis-kryonix',
            port: 6379,
            retry_strategy: (options) => {
                if (options.error && options.error.code === 'ECONNREFUSED') {
                    return new Error('Redis server recusou conex√£o');
                }
                if (options.total_retry_time > 1000 * 60 * 60) {
                    return new Error('Tempo limite de retry excedido');
                }
                if (options.attempt > 10) {
                    return undefined;
                }
                return Math.min(options.attempt * 100, 3000);
            }
        });
    }
    
    // Gerenciar sess√µes mobile
    async createMobileSession(userId, deviceInfo) {
        const sessionId = `mobile:session:${userId}:${Date.now()}`;
        const sessionData = {
            user_id: userId,
            device_type: deviceInfo.type || 'mobile',
            platform: deviceInfo.platform || 'unknown',
            app_version: deviceInfo.appVersion || '1.0.0',
            push_token: deviceInfo.pushToken || '',
            last_activity: new Date().toISOString(),
            geolocation: deviceInfo.location || '',
            language: deviceInfo.language || 'pt-BR',
            timezone: deviceInfo.timezone || 'America/Sao_Paulo'
        };
        
        // Salvar no database 0 (sess√µes)
        await this.client.select(0);
        await this.client.hmset(sessionId, sessionData);
        await this.client.expire(sessionId, 1800); // 30 minutos
        
        return sessionId;
    }
    
    // Cache de dados do usu√°rio
    async cacheUserData(userId, userData) {
        const cacheKey = `cache:user:${userId}`;
        
        await this.client.select(1); // Database cache
        await this.client.hmset(cacheKey, {
            profile_data: JSON.stringify(userData.profile || {}),
            preferences: JSON.stringify(userData.preferences || {}),
            last_sync: new Date().toISOString(),
            cache_version: '1.0'
        });
        await this.client.expire(cacheKey, 3600); // 1 hora
        
        return true;
    }
    
    // Adicionar √† fila de notifica√ß√µes
    async queuePushNotification(notification) {
        await this.client.select(2); // Database filas
        
        const notificationData = {
            user_id: notification.userId,
            title: notification.title,
            body: notification.body,
            data: JSON.stringify(notification.data || {}),
            platform: notification.platform || 'android',
            priority: notification.priority || 'normal',
            created_at: new Date().toISOString()
        };
        
        await this.client.lpush('queue:push', JSON.stringify(notificationData));
        
        return true;
    }
    
    // Incrementar m√©tricas mobile
    async incrementMobileMetrics(metricName, value = 1) {
        await this.client.select(3); // Database m√©tricas
        
        // Incrementar contador
        await this.client.incrby(`metrics:mobile:${metricName}`, value);
        
        // Adicionar √† s√©rie temporal
        const timestamp = Math.floor(Date.now() / 1000);
        await this.client.zadd(`metrics:mobile:${metricName}_timeline`, timestamp, value);
        
        // Manter apenas √∫ltimas 24 horas
        const oneDayAgo = timestamp - 86400;
        await this.client.zremrangebyscore(`metrics:mobile:${metricName}_timeline`, 0, oneDayAgo);
        
        return true;
    }
    
    // Cache de API com TTL inteligente
    async cacheApiResponse(endpoint, data, ttl = 3600) {
        const cacheKey = `api:${endpoint}`;
        
        await this.client.select(6); // Database cache API
        
        const cacheData = {
            data: JSON.stringify(data),
            cached_at: new Date().toISOString(),
            endpoint: endpoint
        };
        
        await this.client.setex(cacheKey, ttl, JSON.stringify(cacheData));
        
        return true;
    }
    
    // Verificar c√≥digo tempor√°rio (WhatsApp, SMS)
    async verifyTemporaryCode(type, identifier, code) {
        const codeKey = `verify:${type}:${identifier}`;
        
        await this.client.select(7); // Database tempor√°rios
        
        const storedCode = await this.client.get(codeKey);
        
        if (storedCode === code) {
            // C√≥digo v√°lido, deletar
            await this.client.del(codeKey);
            return { valid: true, message: 'C√≥digo verificado com sucesso' };
        } else {
            return { valid: false, message: 'C√≥digo inv√°lido ou expirado' };
        }
    }
    
    // Salvar localiza√ß√£o do usu√°rio
    async updateUserLocation(userId, latitude, longitude) {
        await this.client.select(8); // Database geolocaliza√ß√£o
        
        // Adicionar √† estrutura geoespacial
        await this.client.geoadd('locations:users', longitude, latitude, userId);
        
        // Cache do endere√ßo
        const locationKey = `location:user:${userId}`;
        await this.client.hmset(locationKey, {
            latitude: latitude,
            longitude: longitude,
            updated_at: new Date().toISOString()
        });
        await this.client.expire(locationKey, 3600); // 1 hora
        
        return true;
    }
    
    // Buscar usu√°rios pr√≥ximos
    async findNearbyUsers(userId, radiusKm = 10) {
        await this.client.select(8);
        
        // Buscar usu√°rios em um raio
        const nearby = await this.client.georadiusbymember(
            'locations:users', 
            userId, 
            radiusKm, 
            'km', 
            'WITHDIST', 
            'COUNT', 
            50
        );
        
        return nearby;
    }
    
    // Fechar conex√µes
    async disconnect() {
        await this.client.quit();
    }
}

module.exports = KryonixMobileRedis;

// Exemplo de uso:
/*
const redis = new KryonixMobileRedis();

// Criar sess√£o mobile
const sessionId = await redis.createMobileSession('user123', {
    type: 'mobile',
    platform: 'android',
    appVersion: '1.0.0',
    pushToken: 'fcm_token_here',
    location: '-23.5505,-46.6333',
    language: 'pt-BR'
});

// Cache dados do usu√°rio
await redis.cacheUserData('user123', {
    profile: { name: 'Jo√£o', email: 'joao@example.com' },
    preferences: { theme: 'dark', notifications: true }
});

// Enviar notifica√ß√£o
await redis.queuePushNotification({
    userId: 'user123',
    title: 'Nova mensagem',
    body: 'Voc√™ tem uma nova mensagem no KRYONIX',
    platform: 'android',
    priority: 'high'
});
*/
EOF

# === TESTES FINAIS ===
echo "üß™ Executando testes finais..."

# Teste 1: Conectividade Redis
echo "Teste 1: Conectividade Redis..."
docker exec redis-kryonix redis-cli ping || echo "‚ùå Redis n√£o est√° respondendo"

# Teste 2: Databases configurados (incluindo SDK e multi-tenancy)
echo "Teste 2: Verificando databases..."
for db in {0..11}; do
    DB_SIZE=$(docker exec redis-kryonix redis-cli -n $db DBSIZE)
    echo "Database $db: $DB_SIZE chaves"
done

# Teste espec√≠fico SDK
echo "Teste 2.1: Verificando configura√ß√µes SDK..."
SDK_CONFIGS=$(docker exec redis-kryonix redis-cli -n 9 KEYS "sdk:config:*" | wc -l)
echo "Configura√ß√µes SDK: $SDK_CONFIGS clientes"

# Teste espec√≠fico multi-tenancy
echo "Teste 2.2: Verificando tenants..."
TENANTS=$(docker exec redis-kryonix redis-cli -n 10 KEYS "tenant:*" | wc -l)
echo "Tenants configurados: $TENANTS clientes"

# Teste 3: Performance Redis
echo "Teste 3: Testando performance..."
docker exec redis-kryonix redis-cli --latency-history -i 1 -c 5

# Teste 4: IA funcionando
echo "Teste 4: Testando IA..."
python3 /opt/kryonix/scripts/redis-ai-predictive.py

# Teste 5: Backup funcionando
echo "Teste 5: Testando backup..."
/opt/kryonix/scripts/backup-redis.sh

# === MARCAR PROGRESSO ===
echo "4" > /opt/kryonix/.current-part

# === NOTIFICA√á√ÉO FINAL ===
echo "üì± Enviando notifica√ß√£o final..."
curl -X POST "https://evolution.kryonix.com.br/message/sendText" \
  -H "apikey: sua_chave_evolution_api_aqui" \
  -H "Content-Type: application/json" \
  -d '{
    "number": "5517981805327",
    "text": "‚úÖ PARTE-04 CONCLU√çDA!\n\nüîÑ Redis Multi-Tenant otimizado\nüì± 16 databases + SDK + Multi-tenancy\nü§ñ IA analisando multi-tenancy 24/7\nüìä SDK analytics e m√©tricas ativas\nüè¢ Isolamento total por cliente\nüîë API Keys e sess√µes por tenant\nüìä Cria√ß√£o autom√°tica monitorada\nüì± Cache otimizado para apps mobile\nüíæ Backup isolado por cliente\n\nüåê Base para SDK unificado pronta\nüöÄ Arquitetura modular funcionando\nüöÄ Sistema pronto para PARTE-05!"
  }'

echo ""
echo "‚úÖ PARTE-04 CONCLU√çDA COM SUCESSO!"
echo "üîÑ Redis Multi-Tenant otimizado"
echo "üì± 16 databases + SDK + Multi-tenancy"
echo "ü§ñ IA analisando padr√µes por cliente"
echo "üè¢ Isolamento total entre tenants"
echo "üìä SDK analytics em tempo real"
echo "üìä Cria√ß√£o autom√°tica monitorada"
echo ""
echo "üöÄ Pr√≥xima etapa: PARTE-05-TRAEFIK.md"
```

---

## üìã **VALIDA√á√ïES OBRIGAT√ìRIAS**
Ap√≥s executar, confirme se:
- [ ] ‚úÖ Redis respondendo ao comando PING
- [ ] ‚úÖ 16 databases + SDK + multi-tenancy configurados
- [ ] ‚úÖ Estruturas multi-tenant isoladas criadas
- [ ] ‚úÖ IA analisando SDK e tenants automaticamente
- [ ] ‚úÖ Cache isolado por cliente funcionando
- [ ] ‚úÖ M√©tricas SDK e cria√ß√£o autom√°tica
- [ ] ‚úÖ Backup autom√°tico agendado (03:00)
- [ ] ‚úÖ Monitoramento ativo com alertas
- [ ] ‚úÖ Limpeza autom√°tica funcionando
- [ ] ‚úÖ Scripts de integra√ß√£o mobile criados
- [ ] ‚úÖ Notifica√ß√£o WhatsApp enviada

---

**‚ö†Ô∏è IMPORTANTE**: Substitua 'sua_chave_evolution_api_aqui' pela chave real da Evolution API antes de executar.

*ü§ñ Prompt criado pelos 15 Agentes Especializados KRYONIX*
