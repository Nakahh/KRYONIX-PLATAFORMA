#!/usr/bin/env node

/**
 * Script de InstalaÃ§Ã£o AutomÃ¡tica dos Modelos Ollama para KRYONIX
 *
 * Este script baixa e configura automaticamente todas as IAs do mercado
 * disponÃ­veis no Ollama para uso no projeto KRYONIX.
 *
 * Uso:
 * node scripts/setup-ollama-models.js [categoria]
 *
 * Categorias disponÃ­veis:
 * - essential: Modelos essenciais (padrÃ£o)
 * - lightweight: Modelos leves para produÃ§Ã£o
 * - coding: Modelos especializados em programaÃ§Ã£o
 * - conversational: Modelos para conversaÃ§Ã£o
 * - enterprise: Modelos enterprise (grandes)
 * - all: Todos os modelos disponÃ­veis (CUIDADO!)
 */

const { exec } = require("child_process");
const { promisify } = require("util");
const fs = require("fs").promises;
const path = require("path");

const execAsync = promisify(exec);

// ConfiguraÃ§Ã£o
const OLLAMA_URL =
  process.env.OLLAMA_API_URL || "https://apiollama.kryonix.com.br";
const LOG_FILE = path.join(process.cwd(), "ollama-setup.log");

// Modelos organizados por categoria
const MODEL_CATEGORIES = {
  essential: [
    "llama2:7b", // ConversaÃ§Ã£o geral - 3.8GB
    "codellama:7b", // ProgramaÃ§Ã£o - 3.8GB
    "mistral:7b", // Multilingual - 4.1GB
    "orca-mini:3b", // Leve para edge - 1.9GB
    "phi:2.7b", // MatemÃ¡tica/raciocÃ­nio - 1.6GB
    "gemma:2b", // Google lightweight - 1.4GB
  ],

  lightweight: [
    "orca-mini:3b", // 1.9GB
    "phi:2.7b", // 1.6GB
    "gemma:2b", // 1.4GB
    "alpaca:7b", // 3.8GB
  ],

  coding: [
    "codellama:7b", // 3.8GB
    "codellama:13b", // 7.3GB
    "codellama:34b", // 19GB
    "dolphin-mixtral:8x7b", // 26GB (uncensored coding)
  ],

  conversational: [
    "llama2:7b", // 3.8GB
    "llama2:13b", // 7.3GB
    "vicuna:7b", // 3.8GB
    "vicuna:13b", // 7.3GB
    "zephyr:7b", // 4.1GB
  ],

  multilingual: [
    "mistral:7b", // 4.1GB
    "mixtral:8x7b", // 26GB
    "neural-chat:7b", // 4.1GB
  ],

  enterprise: [
    "llama2:70b", // 39GB
    "mixtral:8x7b", // 26GB
    "codellama:34b", // 19GB
    "dolphin-mixtral:8x7b", // 26GB
  ],

  all: [
    // Todos os modelos listados acima
    "llama2:7b",
    "llama2:13b",
    "llama2:70b",
    "codellama:7b",
    "codellama:13b",
    "codellama:34b",
    "mistral:7b",
    "mixtral:8x7b",
    "orca-mini:3b",
    "orca-mini:7b",
    "vicuna:7b",
    "vicuna:13b",
    "alpaca:7b",
    "neural-chat:7b",
    "dolphin-mixtral:8x7b",
    "openhermes:7b",
    "phi:2.7b",
    "zephyr:7b",
    "gemma:2b",
    "gemma:7b",
  ],
};

// InformaÃ§Ãµes detalhadas dos modelos
const MODEL_INFO = {
  "llama2:7b": { size: "3.8GB", desc: "LLaMA 2 7B - Modelo base da Meta" },
  "llama2:13b": { size: "7.3GB", desc: "LLaMA 2 13B - Modelo mÃ©dio" },
  "llama2:70b": { size: "39GB", desc: "LLaMA 2 70B - Modelo grande" },
  "codellama:7b": { size: "3.8GB", desc: "Code Llama 7B - ProgramaÃ§Ã£o" },
  "codellama:13b": {
    size: "7.3GB",
    desc: "Code Llama 13B - ProgramaÃ§Ã£o avanÃ§ada",
  },
  "codellama:34b": {
    size: "19GB",
    desc: "Code Llama 34B - ProgramaÃ§Ã£o expert",
  },
  "mistral:7b": { size: "4.1GB", desc: "Mistral 7B - Modelo europeu" },
  "mixtral:8x7b": { size: "26GB", desc: "Mixtral 8x7B - Mixture of Experts" },
  "orca-mini:3b": { size: "1.9GB", desc: "Orca Mini 3B - Leve e eficiente" },
  "orca-mini:7b": { size: "3.8GB", desc: "Orca Mini 7B - Balanced" },
  "vicuna:7b": { size: "3.8GB", desc: "Vicuna 7B - Conversacional" },
  "vicuna:13b": { size: "7.3GB", desc: "Vicuna 13B - ConversaÃ§Ã£o expert" },
  "alpaca:7b": { size: "3.8GB", desc: "Alpaca 7B - Stanford instruÃ­do" },
  "neural-chat:7b": { size: "4.1GB", desc: "Neural Chat 7B - Intel optimized" },
  "dolphin-mixtral:8x7b": {
    size: "26GB",
    desc: "Dolphin Mixtral - Uncensored",
  },
  "openhermes:7b": {
    size: "3.8GB",
    desc: "OpenHermes 7B - Instruction following",
  },
  "phi:2.7b": {
    size: "1.6GB",
    desc: "Phi 2.7B - Microsoft small but powerful",
  },
  "zephyr:7b": { size: "4.1GB", desc: "Zephyr 7B - Hugging Face tuned" },
  "gemma:2b": { size: "1.4GB", desc: "Gemma 2B - Google lightweight" },
  "gemma:7b": { size: "4.8GB", desc: "Gemma 7B - Google medium" },
};

// UtilitÃ¡rios
async function log(message) {
  const timestamp = new Date().toISOString();
  const logEntry = `[${timestamp}] ${message}\n`;

  console.log(message);
  await fs.appendFile(LOG_FILE, logEntry).catch(() => {});
}

async function checkOllamaRunning() {
  try {
    const response = await fetch(`${OLLAMA_URL}/api/tags`);
    return response.ok;
  } catch (error) {
    return false;
  }
}

async function getInstalledModels() {
  try {
    const response = await fetch(`${OLLAMA_URL}/api/tags`);
    const data = await response.json();
    return data.models ? data.models.map((m) => m.name) : [];
  } catch (error) {
    return [];
  }
}

async function installModel(modelName) {
  const info = MODEL_INFO[modelName];
  await log(
    `ðŸ“¦ Instalando ${modelName} (${info?.size || "Tamanho desconhecido"})...`,
  );
  await log(`    ${info?.desc || "Modelo de IA"}`);

  try {
    const startTime = Date.now();

    // Pull do modelo via API
    const response = await fetch(`${OLLAMA_URL}/api/pull`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        name: modelName,
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    const result = await response.json();
    const duration = Math.round((Date.now() - startTime) / 1000);

    await log(`âœ… ${modelName} instalado com sucesso! (${duration}s)`);
    return true;
  } catch (error) {
    await log(`âŒ Erro ao instalar ${modelName}: ${error.message}`);
    return false;
  }
}

async function testModel(modelName) {
  try {
    await log(`ðŸ§ª Testando ${modelName}...`);

    const response = await fetch(`${OLLAMA_URL}/api/generate`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: modelName,
        prompt: "Responda em portuguÃªs: Como vocÃª estÃ¡ funcionando?",
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    const result = await response.json();
    await log(`âœ… ${modelName} funcionando corretamente!`);
    await log(`    Resposta: ${result.response?.substring(0, 100)}...`);
    return true;
  } catch (error) {
    await log(`âŒ Erro ao testar ${modelName}: ${error.message}`);
    return false;
  }
}

async function calculateTotalSize(models) {
  let totalGB = 0;
  models.forEach((model) => {
    const info = MODEL_INFO[model];
    if (info && info.size) {
      const sizeNum = parseFloat(info.size.replace("GB", ""));
      totalGB += sizeNum;
    }
  });
  return totalGB;
}

async function main() {
  const category = process.argv[2] || "essential";

  await log("ðŸš€ KRYONIX Ollama Models Setup");
  await log("=".repeat(50));

  // Verificar se Ollama estÃ¡ rodando
  await log("ðŸ” Verificando conexÃ£o com Ollama...");
  if (!(await checkOllamaRunning())) {
    await log("âŒ Ollama nÃ£o estÃ¡ rodando ou nÃ£o estÃ¡ acessÃ­vel!");
    await log(`   URL configurada: ${OLLAMA_URL}`);
    await log("   Verifique se o Ollama estÃ¡ instalado e rodando.");
    process.exit(1);
  }
  await log("âœ… Ollama estÃ¡ funcionando!");

  // Verificar categoria
  if (!MODEL_CATEGORIES[category]) {
    await log(`âŒ Categoria "${category}" nÃ£o existe!`);
    await log("Categorias disponÃ­veis:");
    for (const cat of Object.keys(MODEL_CATEGORIES)) {
      const models = MODEL_CATEGORIES[cat];
      const totalSize = models.reduce((sum, model) => {
        const info = MODEL_INFO[model];
        return sum + (info ? parseFloat(info.size.replace("GB", "")) : 0);
      }, 0);
      await log(
        `   ${cat}: ${models.length} modelos (~${totalSize.toFixed(1)}GB)`,
      );
    }
    process.exit(1);
  }

  const modelsToInstall = MODEL_CATEGORIES[category];
  const totalSize = await calculateTotalSize(modelsToInstall);

  await log(`ðŸ“‹ Categoria selecionada: ${category}`);
  await log(`ðŸ“¦ Modelos para instalar: ${modelsToInstall.length}`);
  await log(`ðŸ’¾ EspaÃ§o total estimado: ~${totalSize.toFixed(1)}GB`);

  // Verificar modelos jÃ¡ instalados
  const installedModels = await getInstalledModels();
  const toInstall = modelsToInstall.filter(
    (model) => !installedModels.includes(model),
  );
  const alreadyInstalled = modelsToInstall.filter((model) =>
    installedModels.includes(model),
  );

  if (alreadyInstalled.length > 0) {
    await log(`âœ… JÃ¡ instalados: ${alreadyInstalled.join(", ")}`);
  }

  if (toInstall.length === 0) {
    await log("ðŸŽ‰ Todos os modelos da categoria jÃ¡ estÃ£o instalados!");

    // Testar modelos
    await log("\nðŸ§ª Testando modelos instalados...");
    for (const model of modelsToInstall) {
      await testModel(model);
      await new Promise((resolve) => setTimeout(resolve, 2000)); // Pausa entre testes
    }

    process.exit(0);
  }

  await log(`\nðŸ”„ Modelos para instalar: ${toInstall.join(", ")}`);

  // ConfirmaÃ§Ã£o para categorias grandes
  if (category === "all" || category === "enterprise") {
    await log("âš ï¸  ATENÃ‡ÃƒO: Esta categoria usa muito espaÃ§o em disco!");
    await log("   VocÃª tem certeza? (Ctrl+C para cancelar)");
    await new Promise((resolve) => setTimeout(resolve, 5000));
  }

  // Instalar modelos
  await log("\nðŸ“¥ Iniciando instalaÃ§Ã£o dos modelos...");
  let successCount = 0;
  let failCount = 0;

  for (let i = 0; i < toInstall.length; i++) {
    const model = toInstall[i];

    await log(`\n[${i + 1}/${toInstall.length}] Processando ${model}...`);

    const success = await installModel(model);
    if (success) {
      successCount++;

      // Testar modelo apÃ³s instalaÃ§Ã£o
      await new Promise((resolve) => setTimeout(resolve, 1000));
      await testModel(model);
    } else {
      failCount++;
    }

    // Pausa entre instalaÃ§Ãµes para nÃ£o sobrecarregar
    if (i < toInstall.length - 1) {
      await log("â³ Aguardando antes do prÃ³ximo download...");
      await new Promise((resolve) => setTimeout(resolve, 3000));
    }
  }

  // Resumo final
  await log("\n" + "=".repeat(50));
  await log("ðŸ“Š RESUMO DA INSTALAÃ‡ÃƒO");
  await log(`âœ… Sucessos: ${successCount}`);
  await log(`âŒ Falhas: ${failCount}`);
  await log(`ðŸ“¦ Total instalado: ${successCount}/${toInstall.length}`);

  if (successCount > 0) {
    await log("\nðŸŽ‰ Modelos instalados com sucesso!");
    await log("ðŸ’¡ Para usar os modelos, acesse:");
    await log(`   - Interface Web: ${OLLAMA_URL}`);
    await log("   - API: POST /api/generate");
    await log("   - Painel KRYONIX: /ai/autonomous");
  }

  if (failCount > 0) {
    await log("\nâš ï¸  Alguns modelos falharam na instalaÃ§Ã£o.");
    await log("   Verifique o log para mais detalhes.");
    await log("   VocÃª pode tentar reinstalar individualmente.");
  }

  // Salvar configuraÃ§Ã£o final
  try {
    const finalInstalledModels = await getInstalledModels();
    const config = {
      timestamp: new Date().toISOString(),
      category: category,
      requested: modelsToInstall,
      installed: finalInstalledModels,
      success: successCount,
      failed: failCount,
      totalSize: totalSize,
    };

    await fs.writeFile(
      path.join(process.cwd(), "ollama-installation-report.json"),
      JSON.stringify(config, null, 2),
    );

    await log("\nðŸ’¾ RelatÃ³rio salvo em: ollama-installation-report.json");
  } catch (error) {
    await log(`âŒ Erro ao salvar relatÃ³rio: ${error.message}`);
  }

  await log("\nðŸ Setup concluÃ­do!");
}

// Executar script
if (require.main === module) {
  main().catch(async (error) => {
    await log(`ðŸ’¥ Erro fatal: ${error.message}`);
    process.exit(1);
  });
}

module.exports = {
  MODEL_CATEGORIES,
  MODEL_INFO,
  installModel,
  testModel,
  getInstalledModels,
};
